{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertForQuestionAnswering,\n",
    "    BertTokenizerFast,\n",
    ")\n",
    "from scipy.special import softmax\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the context and the question\n",
    "context = \"The giraffe is a large African hoofed mammal belonging to the genus Giraffa. It is the tallest living terrestrial animal and the largest ruminant on Earth. Traditionally, giraffes were thought to be one species, Giraffa camelopardalis, with nine subspecies. Most recently, researchers proposed dividing them into up to eight extant species due to new research into their mitochondrial and nuclear DNA, as well as morphological measurements. Seven other extinct species of Giraffa are known from the fossil record.\"\n",
    "question = \"How many giraffe species are there?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model name and loading the tokenizer and the model\n",
    "model_name = \"deepset/bert-base-cased-squad2\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the context and the question\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "tokenizer.tokenize(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model and getting the start and end scores\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "start_scores, end_scores = softmax(outputs.start_logits)[0], softmax(outputs.end_logits)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the scores and plotting them\n",
    "scores_df = pd.DataFrame({\n",
    "    \"Token Position\": list(range(len(start_scores))) * 2,\n",
    "    \"Score\": list(start_scores) + list(end_scores),\n",
    "    \"Score Type\": [\"Start\"] * len(start_scores) + [\"End\"] * len(end_scores),\n",
    "})\n",
    "px.bar(scores_df, x=\"Token Position\", y=\"Score\", color=\"Score Type\", barmode=\"group\", title=\"Start and End Scores for Tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the answer from the model\n",
    "start_idx = np.argmax(start_scores)\n",
    "end_idx = np.argmax(end_scores)\n",
    "answer_ids = inputs.input_ids[0][start_idx: end_idx + 1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# Defining a function to predict the answer to a question given a context\n",
    "def predict_answer(context, question):\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    start_scores, end_scores = softmax(outputs.start_logits)[0], softmax(outputs.end_logits)[0]\n",
    "    start_idx = np.argmax(start_scores)\n",
    "    end_idx = np.argmax(end_scores)\n",
    "    confidence_score = (start_scores[start_idx] + end_scores[end_idx]) /2\n",
    "    answer_ids = inputs.input_ids[0][start_idx: end_idx + 1]\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "    if answer != tokenizer.cls_token:\n",
    "        return answer, confidence_score\n",
    "    return None, confidence_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a new context and predicting answers to some questions\n",
    "context = \"\"\"Coffee is a beverage prepared from roasted coffee beans. Darkly colored, bitter, and slightly acidic, coffee has a stimulating effect on humans, primarily due to its caffeine content. It has the highest sales in the world market for hot drinks.[2][unreliable source?]\n",
    "...\n",
    "\"\"\"\n",
    "len(tokenizer.tokenize(context))\n",
    "predict_answer(context, \"What is coffee?\")\n",
    "predict_answer(context, \"What are the most common coffee beans?\")\n",
    "predict_answer(context, \"How can I make ice coffee?\")\n",
    "predict_answer(context[4000:], \"How many people are dependent on coffee for their income?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to chunk sentences\n",
    "def chunk_sentences(sentences, chunk_size, stride):\n",
    "    chunks = []\n",
    "    num_sentences = len(sentences)\n",
    "    for i in range(0, num_sentences, chunk_size - stride):\n",
    "        chunk = sentences[i: i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
